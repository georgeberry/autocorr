---
title: "autocorr"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggraph)
library(tidygraph)
library(foreach)
library(doParallel)
library(mlr)
library(cowplot)
library(caret)
library(xtable)

registerDoParallel(cores=8)

fn_par_bootstrap = function(fn, reps=200, ...) {
  df_results = foreach(
    i=1:reps,
    .combine=rbind
  ) %dopar% {
    result = fn(...)
    result$rep = i
    return(result)
  }
  return(df_results)
}

N_REPS = 100

#### DGPs ########################################################################

fn_dgp_indep = function(size=2000) {
  g = play_barabasi_albert(size, 0.8, 5)
  g = g %>%
    bind_edges(
      g %>%
        activate(edges) %>%
        as_tibble() %>%
        mutate(tmp=to, to=from, from=tmp) %>%
        select(to, from)
    ) %>%
    activate(nodes) %>%
    mutate(
      X = rnorm(n()),
      Y_prob_true = 1 / (1 + exp(- 2 * X)),
      outdeg = centrality_degree(mode='out'),
      outdeg_inv = 1 / outdeg,
      indeg = centrality_degree(mode='in'),
      indeg_inv = 1 / indeg,
      Y = rbinom(
        n(),
        1,
        Y_prob_true
      ),
    ) %>%
    activate(edges) %>%
    mutate(
      Y_ego = .N()$Y[from],
      Y_nbr = .N()$Y[to],
      Y = Y_ego * Y_nbr,
      indeg_ego = .N()$indeg[from],
      indeg_nbr = .N()$indeg[to],
      outdeg_ego = .N()$outdeg[from],
      outdeg_nbr = .N()$outdeg[to],
      indeg_inv_ego = .N()$indeg_inv[from],
      indeg_inv_nbr = .N()$indeg_inv[to],
      outdeg_inv_ego = .N()$outdeg_inv[from],
      outdeg_inv_nbr = .N()$outdeg_inv[to],
      X_ego = .N()$X[from],
      X_nbr = .N()$X[to],
    )
  return(g)
}

fn_dgp_corr = function(size=2000) {
  g = play_barabasi_albert(size, 0.8, 5)
  g = g %>%
    bind_edges(
      g %>%
        activate(edges) %>%
        as_tibble() %>%
        mutate(tmp=to, to=from, from=tmp) %>%
        select(to, from)
    ) %>%
    activate(nodes) %>%
    mutate(
      X = rnorm(n()),
      ZZ = rnorm(n()), 
      Z = map_local_dbl(
        .f = function(neighborhood, ...) {
          mean(as_tibble(neighborhood, active='nodes')$ZZ)
        }
      ),
      Z = (Z - mean(Z)) / sd(Z),
      Y_prob_true = 1 / (1 + exp(- 2 * X - Z)),
      outdeg = centrality_degree(mode='out'),
      outdeg_inv = 1 / outdeg,
      indeg = centrality_degree(mode='in'),
      indeg_inv = 1 / indeg,
      Y = rbinom(
        n(),
        1,
        Y_prob_true
      ),
    ) %>%
    activate(edges) %>%
    mutate(
      Y_ego = .N()$Y[from],
      Y_nbr = .N()$Y[to],
      Y = Y_ego * Y_nbr,
      indeg_ego = .N()$indeg[from],
      indeg_nbr = .N()$indeg[to],
      outdeg_ego = .N()$outdeg[from],
      outdeg_nbr = .N()$outdeg[to],
      indeg_inv_ego = .N()$indeg_inv[from],
      indeg_inv_nbr = .N()$indeg_inv[to],
      outdeg_inv_ego = .N()$outdeg_inv[from],
      outdeg_inv_nbr = .N()$outdeg_inv[to],
      X_ego = .N()$X[from],
      X_nbr = .N()$X[to],
    )
  return(g)
}

fn_dgp_sampling = function(size=2000) {
  g = play_barabasi_albert(size, 0.8, 5)
  g = g %>%
    bind_edges(
      g %>%
        activate(edges) %>%
        as_tibble() %>%
        mutate(tmp=to, to=from, from=tmp) %>%
        select(to, from)
    ) %>%
    activate(nodes) %>%
    mutate(
      X = rnorm(n()),
      Y_prob_true = 1 / (1 + exp(-2*X)),
      outdeg = centrality_degree(mode='out'),
      outdeg_inv = 1 / outdeg,
      indeg = centrality_degree(mode='in'),
      indeg_inv = 1 / indeg,
      Y = rbinom(
        n(),
        1,
        Y_prob_true
      ),
      Q = rnorm(n()),
      # probably we see the true value
      Q_prob = 1 / (1 + exp(-2*Q - X)),
      Q_draw = rbinom(
        n(),
        1,
        Q_prob
      ),
      gt = Q_draw
    ) %>%
    activate(edges) %>%
    mutate(
      Y_ego = .N()$Y[from],
      Y_nbr = .N()$Y[to],
      indeg_ego = .N()$indeg[from],
      indeg_nbr = .N()$indeg[to],
      outdeg_ego = .N()$outdeg[from],
      outdeg_nbr = .N()$outdeg[to],
      indeg_inv_ego = .N()$indeg_inv[from],
      indeg_inv_nbr = .N()$indeg_inv[to],
      outdeg_inv_ego = .N()$outdeg_inv[from],
      outdeg_inv_nbr = .N()$outdeg_inv[to],
      X_ego = .N()$X[from],
      X_nbr = .N()$X[to],
      Q_ego = .N()$Q[from],
      Q_nbr = .N()$Q[to],
      Q_prob_ego = .N()$Q_prob[from],
      Q_prob_nbr = .N()$Q_prob[to],
      gt_ego = .N()$gt[from],
      gt_nbr = .N()$gt[to],
      gt = as.numeric(gt_ego & gt_nbr),
      gt_indep_prob = Q_prob_ego * Q_prob_nbr,
      E=graph_size(),
      outdeg_ego_frac = outdeg_ego / (E),
      indeg_nbr_frac = indeg_nbr / (E)
    )
  return(g)
}
```

## Main results: model comparison

- Node only
- Node with ego weights
- Edge
- Full

```{r}
fn_insamp_bias = function(fn_g) {
  g = fn_g()
  df_nodes = g %>% activate(nodes) %>% as_tibble()
  df_edges = g %>% activate(edges) %>% as_tibble()
  
  # true value
  val_true = sum(df_edges$outdeg_inv_ego * df_edges$Y_ego * df_edges$Y_nbr)
  
  # node model
  df_nodes$Y_basic = predict(
    glm(
      Y ~ X,
      family='binomial',
      data=df_nodes,
    ),
    type='response'
  )
  val_node = sum(
    df_edges$outdeg_inv_ego *
    df_nodes$Y_basic[df_edges$from] *
    df_nodes$Y_basic[df_edges$to]
  )
  
  # degree weight edge model
  df_edges$Y_ego_hat = predict(
    glm(
      Y_ego ~ X_ego + X_nbr + outdeg_inv_ego + outdeg_ego + indeg_nbr,
      family='binomial',
      data=df_edges,
    ),
    type='response'
  )
  df_edges$Y_nbr_hat = predict(
    glm(
      Y_nbr ~ X_ego + X_nbr + outdeg_inv_ego + outdeg_ego + indeg_nbr,
      family='binomial',
      data=df_edges,
    ),
    type='response'
  )
  val_degree_weight = sum(
    df_edges$outdeg_inv_ego *
    df_edges$Y_ego_hat *
    df_edges$Y_nbr_hat
  )
  df_edges$Y_ego_hat = NULL
  df_edges$Y_nbr_hat = NULL

  # ego-then-alter model
  df_edges$Y_ego_hat = predict(
    glm(
      Y_ego ~ X_ego + X_nbr + outdeg_inv_ego + outdeg_ego + indeg_nbr,
      family='binomial',
      data=df_edges,
    ),
    type='response'
  )
  df_edges$Y_nbr_hat = predict(
    glm(
      Y_nbr ~ X_ego + X_nbr + outdeg_inv_ego + outdeg_ego + indeg_nbr + Y_ego_hat,
      family='binomial',
      data=df_edges,
    ),
    type='response'
  )
  val_ego_then_alter = sum(
    df_edges$outdeg_inv_ego *
    df_edges$Y_ego_hat *
    df_edges$Y_nbr_hat
  )
  
  return(
    data.frame(
      val_true=val_true,
      val_node=val_node,
      val_degree_weight=val_degree_weight,
      val_ego_then_alter=val_ego_then_alter
    )
  )
}

df_insamp = bind_rows(
  fn_par_bootstrap(fn_insamp_bias, N_REPS, fn_dgp_indep) %>%
    mutate(
      category='Correct specification'
    ),
  fn_par_bootstrap(fn_insamp_bias, N_REPS, fn_dgp_corr) %>%
    mutate(
      category='Omitted network feature'
    )
) %>%
  mutate(
    category=factor(
      category,
      levels=c('Correct specification', 'Omitted network feature')
    )
  )

df_insamp_wide = bind_rows(
  df_insamp %>%
      mutate(
      `Node model` = abs(val_node - val_true) / val_true,
      `Degree weighted model` = abs(val_degree_weight - val_true) / val_true,
      `Ego-then-alter model` = abs(val_ego_then_alter - val_true) / val_true,
      statistic = 'Expected absolute bias'
    ),
  df_insamp %>%
    mutate(
      `Node model` = (val_node - val_true) / val_true,
      `Degree weighted model` = (val_degree_weight - val_true) / val_true,
      `Ego-then-alter model` = (val_ego_then_alter - val_true) / val_true,
      statistic = 'Systematic bias'
    )
)
```

```{r}
df_insamp_long = df_insamp_wide %>%
  gather(
  key,
  val,
  `Node model`:`Ego-then-alter model`
) %>%
mutate(
  key = factor(
    key,
    levels = c(
      "Node model",
      "Degree weighted model",
      "Ego-then-alter model"
    )
  )
)
```

```{r}
df_insamp_long %>%
  filter(statistic=='Percent bias') %>%
  ggplot(aes(x=key, y=val, color=key)) +
    geom_boxplot() +
    labs(
      title='Error using true model',
      x=element_blank(),
      y='Percent of true value error'
    ) +
    guides(color=FALSE) +
    theme_cowplot(font_size=12) +
    facet_wrap(~category)
```

```{r}
df_insamp_long %>%
  filter(statistic=='Percent absolute error') %>%
  ggplot(aes(x=key, y=val, color=key)) +
    geom_boxplot() +
    labs(
      title='Error using true model',
      x=element_blank(),
      y='Percent of true value error'
    ) +
    guides(color=FALSE) +
    theme_cowplot(font_size=12) +
    facet_wrap(~category)
```

```{r}
df_insamp_wide %>%
  select(
    `Node model`,
    `Degree weighted model`,
    `Ego-then-alter model`,
    Category=category,
    Statistic=statistic
  ) %>%
  group_by(Statistic, Category) %>%
  summarize_if(is.numeric, .funs = list(mean=mean, sd=sd)) %>%
  xtable(digits=4)
```

## Dyadic cross validation

```{r}
fn_demo_cv = function(fn_g, full_mod=TRUE) {
  g = fn_g()
  # omit Z from regressions
  df_nodes = g %>% activate(nodes) %>% as_tibble()
  df_edges = g %>% activate(edges) %>% as_tibble()
  
  df_edges = sample_n(df_edges, 200)
  
  df_result = data.frame(
    Y_true=c(),
    Y_hat=c(),
    R1=c(),
    R2=c()
  )
  folds = createFolds(
    1:nrow(df_edges),
    k=10,
    list=TRUE,
    returnTrain=TRUE
  )
  for (train_idxs in folds) {
    test_idxs = setdiff(1:nrow(df_edges), train_idxs)

    df_train = df_edges[train_idxs,]
    df_test = df_edges[test_idxs,]
    
    ego_mod = glm(
      Y_ego ~ X_ego + X_nbr + outdeg_inv_ego + outdeg_ego + indeg_nbr,
      family='binomial',
      data=df_train
    )
    if (full_mod == TRUE) {
      df_train$Y_ego_hat = predict(
        ego_mod,
        newdata=df_train,
        type='response'
      )
      nbr_mod = glm(
        Y_nbr ~ X_ego + X_nbr + outdeg_inv_ego + outdeg_ego + indeg_nbr + Y_ego_hat,
        family='binomial',
        data=df_train
      )
    } else {
      nbr_mod = glm(
        Y_nbr ~ X_ego + X_nbr + outdeg_inv_ego + outdeg_ego + indeg_nbr,
        family='binomial',
        data=df_train,
      )
    }
    df_test$Y_ego_hat = predict(
      ego_mod,
      newdata=df_test,
      type='response'
    )
    df_test$Y_nbr_hat = predict(
      nbr_mod,
      newdata=df_test,
      type='response'
    )

    df_test$e_ego = df_test$Y_ego_hat - df_test$Y_ego
    df_test$e_nbr = df_test$Y_nbr_hat - df_test$Y_nbr
    Y_true = sum(df_test$outdeg_inv_ego * df_test$Y_ego * df_test$Y_nbr)
    Y_hat = sum(df_test$outdeg_inv_ego * df_test$Y_ego_hat * df_test$Y_nbr_hat)
    R1 = sum(df_test$outdeg_inv_ego * df_test$Y_ego * df_test$e_nbr)
    R2 = sum(df_test$outdeg_inv_ego * df_test$Y_ego_hat * df_test$e_nbr)
    df_result = rbind(
      df_result,
      data.frame(
        Y_true=Y_true,
        Y_hat=Y_hat,
        R1=R1,
        R2=R2
      )
    )
  }
  return(df_result)
}

N_REPS = 100

df_demo_cv = bind_rows(
  fn_par_bootstrap(fn_demo_cv, N_REPS, fn_dgp_corr, full_mod=TRUE) %>%
    mutate(
      category='OV_full'
    ),
  fn_par_bootstrap(fn_demo_cv, N_REPS, fn_dgp_corr, full_mod=FALSE) %>%
    mutate(
      category='OV_basic'
    ),
  fn_par_bootstrap(fn_demo_cv, N_REPS, fn_dgp_indep, full_mod=TRUE) %>%
    mutate(
      category='indep_full'
    ),
  fn_par_bootstrap(fn_demo_cv, N_REPS, fn_dgp_indep, full_mod=FALSE) %>%
    mutate(
      category='indep_basic'
    )
) %>%
  group_by(category, rep) %>%
  summarize_all(.funs=list(mean=mean, sd=sd)) %>%
  ungroup() %>%
  mutate(
    category=case_when(
      category == "indep_basic" ~ 'Degree weights,\ncorrect specification',
      category == "indep_full" ~ 'Ego-then-alter model,\n correct specification',
      category == "OV_basic" ~ 'Degree weights,\nomitted network feature',
      category == 'OV_full' ~ 'Ego-then-alter model,\nomitted network feature'
    ),
    category=factor(
      category,
      levels=c(
        'Degree weights,\ncorrect specification',
        'Ego-then-alter model,\n correct specification',
        'Degree weights,\nomitted network feature',
        'Ego-then-alter model,\nomitted network feature'
      )
    )
  )
```

```{r}
thm = theme_cowplot() +
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    legend.text=element_text(size=8),
    axis.title.y = element_text(size=10),
    legend.title=element_blank()
  ) 

p_demo_cv_mad = df_demo_cv %>%
  mutate(
    mad = abs(Y_hat_mean - Y_true_mean) / Y_true_mean
  ) %>%
  ggplot(aes(x=category, y=mad, color=category)) +
  geom_hline(yintercept=0, linetype='dotdash', alpha=0.4) +
  geom_boxplot() +
  labs(
    x=element_blank(),
    y='Absolute bias',
    title='Expected absolute bias'
  ) +
  thm +
  guides(color=FALSE)


p_demo_cv_bias = df_demo_cv %>%
  mutate(
    bias = (Y_hat_mean - Y_true_mean) / Y_true_mean
  ) %>%
  ggplot(aes(x=category, y=bias, color=category)) +
  geom_hline(yintercept=0, linetype='dotdash', alpha=0.4) +
  geom_boxplot() +
  labs(
    x=element_blank(),
    y='Bias',
    title='Systematic bias'
  ) +
  thm +
  guides(color=FALSE)

p_demo_cv_nuisance = df_demo_cv %>%
  mutate(
    R1=R1_mean,
    R2=R2_mean
  ) %>%
  gather(key, val, R1:R2) %>%
  mutate(
    val=val/Y_true_mean
  ) %>%
  ggplot(aes(x=category, y=val, color=category)) +
  geom_hline(yintercept=0, linetype='dotdash', alpha=0.4) +
  geom_boxplot() +
  labs(
    x=element_blank(),
    y='Term value',
    title='Residual terms'
  ) +
  facet_grid(~key) +
  thm +
  theme(legend.position='bottom')

plot_grid(
  p_demo_cv_mad,
  p_demo_cv_bias,
  p_demo_cv_nuisance,
  ncol=1,
  labels=c('A', 'B', 'C')
)
```

## Directly predict dyads

```{r}
fn_dyadic_cv = function(fn_g) {
  g = fn_g()

  df_edges = g %>% activate(edges) %>% as_tibble()
  
  df_result = data.frame(
    val_true=c(),
    val_basic=c(),
    val_full=c()
  )
  folds = createFolds(
    1:nrow(df_edges),
    k=10,
    list=TRUE,
    returnTrain=TRUE
  )
  
  for (train_idxs in folds) {
    test_idxs = setdiff(1:nrow(df_edges), train_idxs)

    df_train = df_edges[train_idxs,]
    df_test = df_edges[test_idxs,]

    # true value
    val_true = sum(df_test$outdeg_inv_ego * df_test$Y)
    
    # basic model with ego inv deg
    basic_mod = glm(
      Y ~ outdeg_inv_ego,
      family='binomial',
      data=df_train
    )
    basic_preds = predict(
      basic_mod,
      type='response',
      newdata=df_test
    )
    val_basic = sum(df_test$outdeg_inv_ego * basic_preds)
      
    # ego and alter features
    full_mod = glm(
      Y ~ X_ego + X_nbr + outdeg_inv_ego + outdeg_ego + indeg_nbr,
      family='binomial',
      data=df_train,
    )
    full_preds = predict(
      full_mod,
      type='response',
      newdata=df_test
    )
    val_full = sum(df_test$outdeg_inv_ego * full_preds)
    
    df_result = rbind(
      df_result,
      data.frame(
        val_true=val_true,
        val_basic=val_basic,
        val_full=val_full
      )
    )
  }
  return(df_result)
}

df_dyadic_cv = bind_rows(
  fn_par_bootstrap(fn_dyadic_cv, N_REPS, fn_dgp_corr) %>%
    mutate(
      category='Omitted variable'
    ),
  fn_par_bootstrap(fn_dyadic_cv, N_REPS, fn_dgp_indep) %>%
    mutate(
      category='Correct specification'
    )
)
```

## Model comparison

The dyadic cv function already has the two edge-level models.

```{r}
fn_node_cv = function(fn_g) {
  g = fn_g() %>%
    activate(nodes) %>%
    mutate(
      idx=1:n()
    )
  
  df_nodes = g %>% activate(nodes) %>% as_tibble()
  df_edges = g %>% activate(edges) %>% as_tibble()
  
  df_result = data.frame(
    val_true_node=c(),
    val_true_edge=c(),
    val_node_basic=c(),
    val_node_weight=c(),
    val_egoalter=c()
  )
  node_folds = createFolds(
    1:nrow(df_nodes),
    k=10,
    list=TRUE,
    returnTrain=TRUE
  )
  edge_folds = createFolds(
    1:nrow(df_edges),
    k=10,
    list=TRUE,
    returnTrain=TRUE
  )
  
  for (fold_idx in 1:10) {
    train_idxs_nodes = node_folds[[fold_idx]]
    test_idxs_nodes = setdiff(1:nrow(df_nodes), train_idxs_nodes)
    df_nodes_train = df_nodes[train_idxs_nodes,]
    df_nodes_test = df_nodes[test_idxs_nodes,]
    # induced subgraph for nodes in the node test set
    df_nodes_test_g_edges = g %>%
      convert(to_subgraph, idx %in% test_idxs_nodes, subset_by='nodes') %>%
      activate(edges) %>%
      as_tibble()
    
    train_idxs_edges = edge_folds[[fold_idx]]
    test_idxs_edges = setdiff(1:nrow(df_edges), train_idxs_edges)
    df_edges_train = df_edges[train_idxs_edges,]
    df_edges_test = df_edges[test_idxs_edges,]
    
    val_true_node = sum(
      df_nodes_test_g_edges$outdeg_inv_ego *
      df_nodes_test_g_edges$Y_ego *
      df_nodes_test_g_edges$Y_nbr
    )
    val_true_edge = sum(
      df_edges_test$outdeg_inv_ego *
      df_edges_test$Y_ego *
      df_edges_test$Y_nbr
    )
    
    # basic model
    mod_node_basic = glm(
      Y ~ X,
      family='binomial',
      data=df_nodes_train
    )
    Y_hat_node_basic = predict(
      mod_node_basic,
      type='response',
      newdata=df_nodes_test
    )
    val_node_basic = sum(
      df_nodes_test_g_edges$outdeg_inv_ego *
      Y_hat_node_basic[df_nodes_test_g_edges$from] *
      Y_hat_node_basic[df_nodes_test_g_edges$to]
    )
    
    # node weight model
    mod_node_weight_ego = glm(
      Y_ego ~ X_ego + X_nbr + outdeg_inv_ego + outdeg_ego + indeg_nbr,
      family='binomial',
      data=df_edges_train,
    )
    Y_hat_ego_node_weight = predict(
      mod_node_weight_ego,
      type='response',
      newdata=df_edges_test
    )
    mod_node_weight_alter = glm(
      Y_nbr ~ X_ego + X_nbr + outdeg_inv_ego + outdeg_ego + indeg_nbr,
      family='binomial',
      data=df_edges_train,
    )
    Y_hat_nbr_node_weight = predict(
      mod_node_weight_alter,
      type='response',
      newdata=df_edges_test
    )
    val_node_weight = sum(
      df_edges_test$outdeg_inv_ego *
      Y_hat_ego_node_weight *
      Y_hat_nbr_node_weight
    )
    
    # ego-then-alter model
    mod_node_ego_egoalter = glm(
      Y_ego ~ X_ego + X_nbr + outdeg_inv_ego + outdeg_ego + indeg_nbr,
      family='binomial',
      data=df_edges_train
    )
    Y_hat_ego_egoalter = predict(
      mod_node_ego_egoalter,
      type='response',
      newdata=df_edges_test
    )
    df_edges_train$Y_hat_ego = predict(
      mod_node_ego_egoalter,
      type='response',
      newdata=df_edges_train
    )
    df_edges_test$Y_hat_ego = Y_hat_ego_egoalter
    mod_node_nbr_egoalter = glm(
      Y_nbr ~ X_ego + X_nbr + Y_hat_ego + outdeg_inv_ego + outdeg_ego + indeg_nbr,
      family='binomial',
      data=df_edges_train
    )
    Y_hat_nbr_egoalter = predict(
      mod_node_nbr_egoalter,
      type='response',
      newdata=df_edges_test
    )
    val_egoalter = sum(
      df_edges_test$outdeg_inv_ego *
      Y_hat_ego_egoalter *
      Y_hat_nbr_egoalter
    )
    df_result=rbind(
      df_result,
      data.frame(
        val_true_node=val_true_node,
        val_true_edge=val_true_edge,
        val_node_basic=val_node_basic,
        val_node_weight=val_node_weight,
        val_egoalter=val_egoalter
      )
    )
  }
  return(df_result)
}
df_node_cv = bind_rows(
  fn_par_bootstrap(fn_node_cv, N_REPS, fn_dgp_corr) %>%
    mutate(
      category='Omitted variable'
    ),
  fn_par_bootstrap(fn_node_cv, N_REPS, fn_dgp_indep) %>%
    mutate(
      category='Correct specification'
    ),
)
```


```{r}
df_bias_comparison = rbind(
  df_node_cv %>%
    mutate(
      val_node_basic = (val_node_basic - val_true_node),
      val_node_weight = (val_node_weight - val_true_edge),
      val_egoalter = (val_egoalter - val_true_edge)
    ) %>%
    group_by(
      category, rep
    ) %>%
    summarize(
      val_node_basic = mean(val_node_basic) / mean(val_true_node),
      val_node_weight = mean(val_node_weight) / mean(val_true_edge),
      val_egoalter = mean(val_egoalter) / mean(val_true_edge),
    ) %>%
    gather(key, val, val_node_basic:val_egoalter) %>%
    select(
      key, val, category
    ),
  df_dyadic_cv %>%
    mutate(
      val_basic = (val_basic - val_true),
      val_full = (val_full - val_true)
    ) %>%
    group_by(
      category, rep
    ) %>%
    summarize(
      val_basic = mean(val_basic) / mean(val_true),
      val_full = mean(val_full) / mean(val_true)
    ) %>%
    gather(key, val, val_basic:val_full) %>%
    select(
      key, val, category
    )
) %>%
  mutate(
    key=factor(
      case_when(
        key == "val_node_basic" ~ 'Node model',
        key == "val_node_weight" ~ 'Weighted node\nmodel',
        key == "val_egoalter" ~ 'Ego then\nalter model',
        key == 'val_full' ~ 'Edge model',
        key == 'val_basic' ~ 'Inverse degree\nonly edge model'
      ),
      levels=c(
        'Node model',
        'Weighted node\nmodel',
        'Ego then\nalter model',
        'Inverse degree\nonly edge model',
        'Edge model'
      )
    )
  )

df_mae_comparison = rbind(
  df_node_cv %>%
    mutate(
      val_node_basic = abs(val_node_basic - val_true_node),
      val_node_weight = abs(val_node_weight - val_true_edge),
      val_egoalter = abs(val_egoalter - val_true_edge)
    ) %>%
    group_by(
      category, rep
    ) %>%
    summarize(
      val_node_basic = mean(val_node_basic) / mean(val_true_node),
      val_node_weight = mean(val_node_weight) / mean(val_true_edge),
      val_egoalter = mean(val_egoalter) / mean(val_true_edge),
    ) %>%
    gather(key, val, val_node_basic:val_egoalter) %>%
    select(
      key, val, category
    ),
  df_dyadic_cv %>%
    mutate(
      val_basic = abs(val_basic - val_true),
      val_full = abs(val_full - val_true)
    ) %>%
    group_by(
      category, rep
    ) %>%
    summarize(
      val_basic = mean(val_basic) / mean(val_true),
      val_full = mean(val_full) / mean(val_true)
    ) %>%
    gather(key, val, val_basic:val_full) %>%
    select(
      key, val, category
    )
) %>%
  mutate(
    key=factor(
      case_when(
        key == "val_node_basic" ~ 'Node model',
        key == "val_node_weight" ~ 'Weighted node\nmodel',
        key == "val_egoalter" ~ 'Ego then\nalter model',
        key == 'val_full' ~ 'Edge model',
        key == 'val_basic' ~ 'Inverse degree\nonly edge model'
      ),
      levels=c(
        'Node model',
        'Weighted node\nmodel',
        'Ego then\nalter model',
        'Inverse degree\nonly edge model',
        'Edge model'

      )
    )
  )
```


```{r}
p_mae_combined_pair = df_mae_comparison %>%
  filter(
    key %in% c('Node model', 'Ego then\nalter model')
  ) %>%
  ggplot(aes(x=key, y=val, color=key)) +
  geom_hline(yintercept=0, linetype='dotdash', alpha=0.4) +
  geom_boxplot() +
  facet_wrap(~category) +
  theme_cowplot(font_size=14) +
  labs(
    x=element_blank(),
    y='Absolute bias',
    title='Expected absolute bias'
  ) +
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    # legend.position=c(0.7,0.8),
    legend.title=element_blank(),
    legend.text=element_text(size=10)
    # axis.text.x = element_text(angle = 45, hjust = 1)
  )

p_bias_combined_pair = df_bias_comparison %>%
  filter(key %in% c('Node model', 'Ego then\nalter model')) %>%
  ggplot(aes(x=key, y=val, color=key)) +
  geom_hline(yintercept=0, linetype='dotdash', alpha=0.4) +
  geom_boxplot() +
  facet_wrap(~category) +
  theme_cowplot(font_size=14) +
  labs(
    x=element_blank(),
    y='Bias',
    title='Systematic bias'
  ) +
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    # legend.position=c(0.7,0.8),
    legend.title=element_blank(),
    legend.text=element_text(size=10)
    # axis.text.x = element_text(angle = 45, hjust = 1)
  )

plot_grid(
  p_mae_combined_pair,
  p_bias_combined_pair,
  ncol=1,
  labels=c('A', 'B')
)
```





```{r}
p_mae_combined_four = df_mae_comparison %>%
  filter(
    key != 'Node model'
  ) %>%
  ggplot(aes(x=key, y=val, color=key)) +
  geom_hline(yintercept=0, linetype='dotdash', alpha=0.4) +
  geom_boxplot() +
  facet_wrap(~category) +
  theme_cowplot(font_size=14) +
  labs(
    x=element_blank(),
    y='Absolute bias',
    title='Expected absolute bias'
  ) +
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    # legend.position=c(0.7,0.8),
    legend.title=element_blank(),
    legend.text=element_text(size=10)
    # axis.text.x = element_text(angle = 45, hjust = 1)
  )

p_bias_combined_four = df_bias_comparison %>%
  filter(
    key != 'Node model'
  ) %>%
  ggplot(aes(x=key, y=val, color=key)) +
  geom_hline(yintercept=0, linetype='dotdash', alpha=0.4) +
  geom_boxplot() +
  facet_wrap(~category) +
  theme_cowplot(font_size=14) +
  labs(
    x=element_blank(),
    y='Bias',
    title='Systematic bias'
  ) +
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    # legend.position=c(0.7,0.8),
    legend.title=element_blank(),
    legend.text=element_text(size=10)
    # axis.text.x = element_text(angle = 45, hjust = 1)
  )

plot_grid(
  p_mae_combined_four,
  p_bias_combined_four,
  ncol=1,
  labels=c('A', 'B')
)
```


## Variance sampling 1% of edges (compare with the cross val demo)

```{r}
fn_samp_true = function(fn_g) {
  g = fn_g()
  res = g %>%
    activate(edges) %>%
    as_tibble() %>%
    mutate(
      val_true=sum(outdeg_inv_ego * Y_ego * Y_nbr)
    ) %>%
    sample_frac(0.01) %>%
    summarize(res = sum(outdeg_inv_ego * Y_ego * Y_nbr) / (0.01 * mean(val_true)))
  return(res)
}
```

```{r}
df_tmp = fn_par_bootstrap(fn_samp_true, N_REPS, fn_dgp_corr)
```

```{r}
hist(df_tmp$res)
```


```{r}
fn_var_comparison_cv = function(fn_g) {
  g = fn_g()
  # omit Z from regressions
  df_nodes = g %>% activate(nodes) %>% as_tibble()
  df_edges = g %>% activate(edges) %>% as_tibble()
  
  df_train = sample_n(df_edges, 400)
  sample_frac = 400 / nrow(df_edges)

  ego_mod = glm(
    Y_ego ~ X_ego + outdeg_inv_ego + outdeg_ego + indeg_nbr,
    family='binomial',
    data=df_train
  )
  df_train$Y_ego_full_edge = predict(
    ego_mod,
    type='response'
  )
  nbr_mod = glm(
    Y_nbr ~ X_nbr + Y_ego_full_edge + outdeg_inv_ego + outdeg_ego + indeg_nbr,
    family='binomial',
    data=df_train
  )
  df_edges$Y_ego_full_edge = predict(
    ego_mod,
    type='response',
    newdata=df_edges
  )
  df_edges$Y_nbr_full_edge = predict(
    nbr_mod,
    type='response',
    newdata=df_edges
  )
  Y_true = sum(df_edges$outdeg_inv_ego * df_edges$Y_ego * df_edges$Y_nbr)
  Y_hat = sum(df_edges$outdeg_inv_ego * df_edges$Y_ego_full_edge * df_edges$Y_nbr_full_edge)

  df_result = data.frame(
    Y_true=Y_true,
    Y_hat=Y_hat,
    Y_direct=(1 / sample_frac) * sum(df_train$outdeg_inv_ego * df_train$Y_ego * df_train$Y_nbr)
  )
  return(df_result)
}


df_demo_var = bind_rows(
  fn_par_bootstrap(fn_var_comparison_cv, N_REPS, fn_dgp_corr) %>%
    mutate(
      category='OV'
    ),
  fn_par_bootstrap(fn_var_comparison_cv, N_REPS, fn_dgp_indep) %>%
    mutate(
      category='indep'
    )
)
```

```{r}
df_demo_var %>%
  mutate(
    Y_hat = abs(Y_hat - Y_true) / Y_true,
    Y_direct = abs(Y_direct - Y_true) / Y_true
  ) %>%
  group_by(
    category
  ) %>%
  gather(key, val, Y_hat:Y_direct) %>%
  ggplot(aes(x=key, y=val)) +
    geom_boxplot() +
    facet_wrap(~category)
```

```{r}
df_demo_var %>%
  mutate(
    Y_hat = (Y_hat - Y_true) / Y_true,
    Y_direct = (Y_direct - Y_true) / Y_true
  ) %>%
  group_by(
    category
  ) %>%
  gather(key, val, Y_hat:Y_direct) %>%
  ggplot(aes(x=key, y=val)) +
    geom_boxplot() +
    facet_wrap(~category)
```


## Simple probability sampling example

The theoretical hunch i have is that we only need to control for the ego weights in the ego model, and alter weights in the alter model

intuition is based on only needing to control for 

```{r}
fn_dgp_ipw = function(size=2000) {
  g = play_barabasi_albert(size, 0.8, 5)
  g = g %>%
    bind_edges(
      g %>%
        activate(edges) %>%
        as_tibble() %>%
        mutate(tmp=to, to=from, from=tmp) %>%
        select(to, from)
    ) %>%
    activate(nodes) %>%
    mutate(
      X = rnorm(n()),
      Y_prob_true = 1 / (1 + exp(- X)),
      outdeg = centrality_degree(mode='out'),
      outdeg_inv = 1 / outdeg,
      indeg = centrality_degree(mode='in'),
      indeg_inv = 1 / indeg,
      Y = rbinom(
        n(),
        1,
        Y_prob_true
      ),
      Q = rnorm(n()),
      # probably we see the true value
      Q_prob = 1 / (1 + exp(1 - Q - X)),
      Q_draw = rbinom(
        n(),
        1,
        Q_prob
      ),
      gt = as.integer(Q_draw == 1)
    ) %>%
    activate(edges) %>%
    mutate(
      Y_ego = .N()$Y[from],
      Y_nbr = .N()$Y[to],
      indeg_ego = .N()$indeg[from],
      indeg_nbr = .N()$indeg[to],
      outdeg_ego = .N()$outdeg[from],
      outdeg_nbr = .N()$outdeg[to],
      indeg_inv_ego = .N()$indeg_inv[from],
      indeg_inv_nbr = .N()$indeg_inv[to],
      outdeg_inv_ego = .N()$outdeg_inv[from],
      outdeg_inv_nbr = .N()$outdeg_inv[to],
      X_ego = .N()$X[from],
      X_nbr = .N()$X[to],
      Q_draw_ego = .N()$Q_draw[from],
      Q_draw_nbr = .N()$Q_draw[to],
      Q_prob_ego = .N()$Q_prob[from],
      Q_prob_nbr = .N()$Q_prob[to],
      gt_ego = .N()$gt[from],
      gt_nbr = .N()$gt[to]
    )
  return(g)
}
```


## AUCs and performance

```{r}
df_nodes = fn_dgp_corr() %>%
  activate(nodes) %>%
  as_tibble()
```

```{r}
tmp = df_nodes %>%
  select(
    Y,
    X,
    Z
  )

tsk = makeClassifTask(data=tmp, target="Y")
lrn = makeLearner("classif.logreg", predict.type='prob')
cv = makeResampleDesc("CV", iters=10)
res_wrong = resample(lrn, tsk, cv, measures=list(acc, auc))
```

```{r}
tmp = df_nodes %>%
  select(
    Y,
    X
  )

tsk = makeClassifTask(data=tmp, target="Y")
lrn = makeLearner("classif.logreg", predict.type='prob')
cv = makeResampleDesc("CV", iters=10)
res_wrong = resample(lrn, tsk, cv, measures=list(acc, auc))
```

```{r}
tmp = df_nodes %>%
  select(
    Y,
    X,
    outdeg_inv
  )

tsk = makeClassifTask(data=tmp, target="Y")
lrn = makeLearner("classif.logreg", predict.type='prob')
cv = makeResampleDesc("CV", iters=10)
res_wrong = resample(lrn, tsk, cv, measures=list(acc, auc))
```